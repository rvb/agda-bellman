open import Agda.Builtin.Bool
open import Agda.Builtin.Equality
open import Agda.Builtin.Float
open import Agda.Builtin.Nat
open import Agda.Builtin.TrustMe --meh

--Lift a boolean test to a proof term (From Idris standard library)
data So : Bool -> Set where
  oh : So true

--Vectors: Length-indexed lists.
data Vect (A : Set) : Nat → Set where
  [] : Vect A zero
  _,-_ : ∀ {n} → A → Vect A n → Vect A (suc n)

data Zero : Set where

--Decidable predicates
data Dec (X : Set) : Set where
  yes : X → Dec X
  no : (X → Zero) → Dec X

--Reflexive decidable equality.
record DecEq (X : Set) : Set where
  field
    decEq : (x y : X) → Dec (x ≡ y)
    --Idris doesn't demand this, we do.
    decEqRefl : (x : X) → (decEq x x ≡ yes refl)

record Σ (X : Set) (f : X → Set) : Set where
  constructor _,_
  field
    fst : X
    snd : f fst

--Problem description, consisting of a state space,
--a state-dependent control, a state transition function
--and a reward function.
record SDP : Set₁ where
  field
    State : Set
    Control : State → Set
    step : (x : State) → (Control x) → State
    reward : (x : State) → (Control x) → State → Float

open SDP

_+F_ = primFloatPlus

--Control sequences starting in a given state, of an indicated length,
--for a specified SDP.
data CtrlSeq (problem : SDP) : (State problem) → Nat → Set where
  Nil : { x : State problem } → CtrlSeq problem x zero
  _::_ : ∀ {n} → {x : State problem } → (y : Control problem x) → CtrlSeq problem (step problem x y) n → CtrlSeq problem x (suc n)


--The reward earned by a control sequence
val : ∀ {p x n} → CtrlSeq p x n → Float
val Nil = 0.0
val {p} {x} (y :: ys) = reward p x y (step p x y) +F  val ys

_||_ : Bool → Bool → Bool
false || y = y
true || y = true

_≼_ : Float → Float → Bool
x ≼ y = primFloatEquality x y || primFloatNumericalLess x y

--Optimality of control sequences.
--A control sequence is optimal if every other control sequence
--earns at most as much as this one (for the same length and starting state)
OptCtrlSeq : ∀ {p x n } → CtrlSeq p x n → Set
OptCtrlSeq {p} {x} {n} s = (s' : CtrlSeq p x n) → So (val s' ≼ val s)

--The empty sequence is always optimal (it's the only sequence of length zero, so the equality holds trivially).
--Fortunately, since we know both sequences are Nil, the vals either side of ≼ reduce,
--so we are left with a goal of type So true, which is oh-so-easy.
nilIsOptCtrlSeq : ∀ { p x } → OptCtrlSeq {p} {x} {zero} Nil
nilIsOptCtrlSeq Nil = oh

--Policies, which choose a (deterministic) control in every state.
Policy : (p  : SDP) → Set
Policy p = (x : State p) → Control p x

PolicySeq : SDP → Nat → Set
PolicySeq p = Vect (Policy p)

--Compute the control sequence implied by a policy sequence, starting in a given state.
ctrls : ∀ {n} → (p : SDP) (x : State  p) → PolicySeq p n → CtrlSeq p x n
ctrls _ x [] = Nil
ctrls prob x (p ,- ps) = p x :: ctrls prob (step prob x (p x)) ps

--Reward generated by a policy sequence
pval : ∀ {n} → (p : SDP) → (x : State p) → PolicySeq p n → Float
pval _ _ [] = 0.0
pval prob x (p ,- ps) = reward prob x (p x) x' +F pval prob x' ps
  where x' = step prob x (p x)

--Policy sequences are optimal if they cannot be improved on for any starting state:
OptPolicySeq : (p : SDP) → (n : Nat) → PolicySeq p n → Set
OptPolicySeq p n ps = (x : State p) → (ps' : PolicySeq p n) → So (pval p x ps' ≼ pval p x ps)


--The value of a policy is the value of its controls.
val_pval_lemma : ∀ {n} → (p : SDP) (x : State p) (ps : PolicySeq p n) → pval p x ps ≡ val (ctrls p x ps)
val p pval x lemma [] = refl
val prob pval x lemma (p ,- pol) rewrite val_pval_lemma prob (step prob x (p x)) pol = refl

--Step 1 in the proof of OptLemma is to construct a policy sequence which matches a given control sequence.
--This requires a few ingredients:
--  -An existing policy
--  -A reflexive decidable equality on X.
--The idea of policyify is to walk down the control/policy sequence, and
--check the state given. If it's the one we have a control for, use that, otherwise fall back
--to the base policy.
--Since we only care about the behaviour starting from the given state anyway, how we fill in
--other states doesn't matter at all.
tweakPolicy : (prob : SDP) → {x : State prob} → (de : DecEq (State prob)) → ((x : State prob) → Control prob x) → (c : Control prob x) → Σ ((y : State prob) → Control prob y) (λ pol → pol x ≡ c)
tweakPolicy prob {x} de pol c = pol' , pol'-correct
  where
    pol'helper : (y : State prob) → (Dec (x ≡ y)) → Control prob y
    pol'helper y (yes refl) = c
    pol'helper y (no cntra) = pol y

    pol' : (y : State prob) → Control prob y
    pol' y = pol'helper y (DecEq.decEq de x y)

    pol'-correct : pol'helper x (DecEq.decEq de x x) ≡ c
    pol'-correct rewrite DecEq.decEqRefl de x = refl

cong : {A B : Set} {x y : A} → (f : A → B) → (x ≡ y) → (f x ≡ f y)
cong f refl = refl

policyify : ∀ {n} → (p : SDP) → (DecEq (State p)) → {x : State p} → (PolicySeq p n) → (cs : CtrlSeq p x n) → Σ (PolicySeq p n) (λ pol → ctrls p x pol ≡ cs)
policyify prob de [] Nil = [] , refl
policyify prob de (p ,- ps) (c :: cs) with tweakPolicy prob de p c | policyify prob de ps cs
policyify prob de (p ,- ps) (.(polhead _) :: cs) | polhead , refl | poltail , prftail = (polhead ,- poltail) , cong _ prftail

sym : ∀ {A : Set} → {x y : A} → x ≡ y → y ≡ x
sym refl = refl

--We can now prove OptLemma.
OptLemma : (p : SDP) (de : DecEq (State p)) (n : Nat) (ps : PolicySeq p n) (ops : OptPolicySeq p n ps) (x : State p) → (OptCtrlSeq (ctrls p x ps))
OptLemma p de n ps opt x ys' with policyify p de ps ys'
OptLemma p de n ps opt x ys' | ps' , prf rewrite sym prf | sym (val_pval_lemma p x ps) | sym (val_pval_lemma p x ps') = opt x ps'

--Nil, being the only zero-length policy, is trivially optimal.
nilIsOptPolicySeq : ∀ {p} → OptPolicySeq p zero []
nilIsOptPolicySeq s [] = oh

--Optimal extension of a policy.
--This basically encodes taking the locally-optimal control
OptExt : ∀ {n} → (p : SDP) → PolicySeq p n → Policy p → Set
OptExt prob ps p = (p' : Policy prob) → (x : State prob) → So (pval prob x (p' ,- ps) ≼ pval prob x (p ,- ps))

--We require monotonicity of ≼. This is essentially an axiom, so we force Agda to accept it.
≼rmonotonic : ∀ x a b → So (a ≼ b) → So ((x +F a) ≼ (x +F b))
≼rmonotonic x a b val with primTrustMe {x = So ((x +F a) ≼ (x +F b))} {y = So true}
... | prf rewrite prf = oh

--Similarly, we require transitivity.
≼trans : ∀ a b c → So (a ≼ b) → So (b ≼ c) → So (a ≼ c)
≼trans a b c a≼b b≼c with primTrustMe {x = So (a ≼ c)} {y = So true}
... | prf rewrite prf = oh

Bellman : ∀ {p n} → (ps : PolicySeq p n) → OptPolicySeq p n ps → (pol : Policy p) → OptExt p ps pol → OptPolicySeq p (suc n) (pol ,- ps)
Bellman {p} ps opt pol optext x (pol' ,- ps') with opt (step p x (pol' x)) ps' | optext pol' x 
... | opttail | opthead =
  ≼trans (pval p x (pol' ,- ps')) _ _ (≼rmonotonic (reward p x (pol' x) _) _ _ opttail) opthead

--Postulate a solution optExt to compute optimal extensions
Extender : SDP → Set
Extender p = ∀ {n} → (ps : PolicySeq p n) → Σ (Policy p) (OptExt p ps)

--Provably-correct backwards induction.
--Note that this proof is generic in the extension mechanism.
backwardsInduction : (p : SDP) → (optExt : Extender p) → (n : Nat) → Σ (PolicySeq p n) (OptPolicySeq p n)
backwardsInduction p optExt zero = [] , nilIsOptPolicySeq
backwardsInduction p optExt (suc n) with backwardsInduction p optExt n | optExt (Σ.fst (backwardsInduction p optExt n))
backwardsInduction p optExt (suc n) | ps , psopt | ext , extopt = (ext ,- ps) , Bellman ps psopt ext extopt

--To implement an Extender, we need a way to maximise over controls.
--To do so, we use the following record of functions:
record Maximiser (State : Set) (Control : State → Set) : Set where
  field
    argmax : (x : State) → (f : Control x → Float) → Σ (Control x) (\am → (y : Control x) → So (f y ≼ f am))

open Maximiser

--An extender:
optExt : (p : SDP) → Maximiser (State p) (Control p) → Extender p
optExt p mx ps = pol , pol-correct
  where
    value : (x : State p) → Control p x → Float
    value x y = reward p x y (step p x y) +F pval p (step p x y) ps    
  
    pol : Policy p
    pol x = Σ.fst (argmax mx x (value x))

    pol-correct : OptExt p ps pol
    pol-correct pol' x with argmax mx x (value x)
    pol-correct pol' x | am , prf = prf (pol' x)

--So.. with this, we can solve any problem (with provably-correct solutions!) provided
--we can provide a Maximiser for it.
